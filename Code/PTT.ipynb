{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import monpa\n",
    "from monpa import utils\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_30704\\3538322082.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  transaction_df = pd.read_csv(\"bda2024_微股力_個股交易數據-2年.csv\")\n"
     ]
    }
   ],
   "source": [
    "ptt_df = pd.read_csv(\"bda2024_202203-202402_討論數據_ptt.csv\")\n",
    "transaction_df = pd.read_csv(\"bda2024_微股力_個股交易數據-2年.csv\")\n",
    "with open('stopwords_zh_ptt.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df['stock_symbol'] = transaction_df['stock_symbol'].astype(str)\n",
    "newsSet = ptt_df[ptt_df['title'].str.contains('聯發科') | ptt_df['content'].str.contains('聯發科')].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_date(input_date, dates_list):\n",
    "    index = sum(1 for d in dates_list if d < input_date)\n",
    "    if index >= len(dates_list):\n",
    "        return -1, -1\n",
    "    else:\n",
    "        return index, dates_list[index]\n",
    "\n",
    "def get_answer(stock_name, input_date, n_days_after, threshold):\n",
    "    dates_list = sorted(transaction_df.loc[(transaction_df['stock_name']==stock_name)]['date'])\n",
    "    i, _ = get_next_date(input_date, dates_list)\n",
    "\n",
    "    if i == -1 or i >= len(dates_list) or i + n_days_after >= len(dates_list):\n",
    "        return 2\n",
    "    \n",
    "    open_date = dates_list[i]\n",
    "    close_date = dates_list[i + n_days_after]\n",
    "    \n",
    "    open_price = transaction_df.loc[(transaction_df['date']==open_date) & (transaction_df['stock_name']==stock_name)]['open'].iloc[0]\n",
    "    close_price = transaction_df.loc[(transaction_df['date']==close_date) & (transaction_df['stock_name']==stock_name)]['close'].iloc[0]\n",
    "    price_change = (close_price - open_price) / open_price\n",
    "\n",
    "    if price_change >= threshold:\n",
    "        return 1     # buy\n",
    "    elif price_change <= -threshold:\n",
    "        return -1    # sell\n",
    "    else:\n",
    "        return 0     # hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsSet['stock_name'] = \"聯發科\"\n",
    "newsSet['category'] = newsSet.apply(lambda r: get_answer(r['stock_name'], r['post_time'], 10, 0.02), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newsSet.pkl', 'wb') as file: \n",
    "    pickle.dump(newsSet, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分出漲跌資料集\n",
    "upSet = newsSet[newsSet['category'] == 1]\n",
    "downSet = newsSet[newsSet['category'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_tf_counter=Counter()\n",
    "up_df_counter=Counter()\n",
    "\n",
    "for index, row in upSet.iterrows():\n",
    "    query = row['title'] + ' ' + row['content']\n",
    "    df_tmp = Counter()\n",
    "    sentence_list = utils.short_sentence(query)\n",
    "\n",
    "    for item in sentence_list:\n",
    "      result_cut = monpa.cut(item) \n",
    "      for term in result_cut:\n",
    "        term = term.strip()\n",
    "        if (len(term) > 1):\n",
    "          up_tf_counter[term] += 1 \n",
    "          if(df_tmp[term] == 0): \n",
    "            df_tmp[term] = 1 \n",
    "    up_df_counter += df_tmp\n",
    "\n",
    "up_tf_idf = {}\n",
    "d = len(upSet)\n",
    "for term, freq in up_tf_counter.items():\n",
    "    up_tf_idf[term] = freq * math.log(d / up_tf_counter[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_tf_counter=Counter()\n",
    "down_df_counter=Counter()\n",
    "\n",
    "for index, row in downSet.iterrows():\n",
    "    query = row['title'] + ' ' + row['content']\n",
    "    df_tmp = Counter()\n",
    "    sentence_list = utils.short_sentence(query)\n",
    "\n",
    "    for item in sentence_list:\n",
    "      result_cut = monpa.cut(item) \n",
    "      for term in result_cut:\n",
    "        term = term.strip()\n",
    "        if (len(term) > 1):\n",
    "          down_tf_counter[term] += 1 \n",
    "          if(df_tmp[term] == 0): \n",
    "            df_tmp[term] = 1 \n",
    "    down_df_counter += df_tmp\n",
    "\n",
    "down_tf_idf = {}\n",
    "d = len(downSet)\n",
    "for term, freq in down_tf_counter.items():\n",
    "    down_tf_idf[term] = freq * math.log(d / down_tf_counter[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Ratio selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>p_type</th>\n",
       "      <th>s_name</th>\n",
       "      <th>s_area_name</th>\n",
       "      <th>post_time</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>page_url</th>\n",
       "      <th>stock_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1646132549195_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2022-03-01 18:56:10.000</td>\n",
       "      <td>[情報] 2323 中環處分有價證券(發哥</td>\n",
       "      <td>addy7533967</td>\n",
       "      <td>1. 標題：公告本公司處分有價證券\\n\\n2. 來源：公開資訊觀測站\\n\\n3. 網址：ht...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1646132174.A.167...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1646140407607_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2022-03-01 21:11:34.000</td>\n",
       "      <td>[新聞] 聯發科全力搶攻5G手機 天璣8100、天璣800</td>\n",
       "      <td>Ruo5566</td>\n",
       "      <td>聯發科全力搶攻5G手機 天璣8100、天璣8000登場\\nhttps://ec.ltn.co...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1646140301.A.983...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1646204116939_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2022-03-02 14:52:08.000</td>\n",
       "      <td>[情報] 0302上市投信買賣超排行</td>\n",
       "      <td>MOMO0478</td>\n",
       "      <td>買超                    賣超\\n名次    股票名稱        超張...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1646203930.A.51D...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1646209462036_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2022-03-02 16:20:45.000</td>\n",
       "      <td>[情報] 0302上市外資買賣超排行</td>\n",
       "      <td>MOMO0478</td>\n",
       "      <td>買超                            賣超\\n名次    股票名稱  ...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1646209247.A.D2B...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1646217643615_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2022-03-02 18:37:59.000</td>\n",
       "      <td>[情報] 0302八大公股銀行買賣超排行</td>\n",
       "      <td>addy7533967</td>\n",
       "      <td>手機介面圖片好讀版：\\n\\n\\n以下資訊依張數排列\\n買超                 ...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1646217502.A.CF6...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1856</td>\n",
       "      <td>1706688439506_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2024-01-31 16:06:51.000</td>\n",
       "      <td>[情報] 0131 上市外資買賣超排行</td>\n",
       "      <td>paidzou</td>\n",
       "      <td>1. 標題：0131 上市外資買賣超排行\\n\\n2. 來源：TWSE\\n\\n3. 網址：ht...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1706688414.A.A04...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1857</td>\n",
       "      <td>1706693007636_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2024-01-31 17:22:06.000</td>\n",
       "      <td>[新聞]  〈聯發科法說〉去年Q4獲利登五季新高</td>\n",
       "      <td>xlaws1987</td>\n",
       "      <td>原文連結：https://reurl.cc/lgzzXd\\n發布時間： 2024-01-31...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1706692928.A.B95...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>1858</td>\n",
       "      <td>1706706929898_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2024-01-31 21:14:40.000</td>\n",
       "      <td>[新聞] 聯發科旗下奕微科車規晶片、MEMS 相繼問</td>\n",
       "      <td>NerfMePls</td>\n",
       "      <td>原文標題：聯發科旗下奕微科車規晶片、MEMS 相繼問世 搶攻全球車廠訂單\\n\\n原文連結：h...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1706706882.A.E24...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1859</td>\n",
       "      <td>1706708637116_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2024-01-31 21:42:06.000</td>\n",
       "      <td>[情報] 113年01月31日信用交易統計</td>\n",
       "      <td>steward135</td>\n",
       "      <td>1. 標題：113年01月31日信用交易統計\\n2. 來源：臺灣證券交易所、證券櫃檯買賣中心...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1706708529.A.276...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1860</td>\n",
       "      <td>1706710789315_PTT02R</td>\n",
       "      <td>bbs</td>\n",
       "      <td>Ptt</td>\n",
       "      <td>Stock</td>\n",
       "      <td>2024-01-31 22:14:25.000</td>\n",
       "      <td>[新聞] 胡潤世界500強企業公布！台積電「被」列</td>\n",
       "      <td>lspss93009</td>\n",
       "      <td>原文標題：胡潤世界500強企業公布！台積電「被」列中國第一　台灣3家入榜\\n\\n原文連結：h...</td>\n",
       "      <td>http://www.ptt.cc/bbs/Stock/M.1706710467.A.F21...</td>\n",
       "      <td>聯發科</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                    id p_type s_name s_area_name  \\\n",
       "0         0  1646132549195_PTT02R    bbs    Ptt       Stock   \n",
       "1         1  1646140407607_PTT02R    bbs    Ptt       Stock   \n",
       "2         2  1646204116939_PTT02R    bbs    Ptt       Stock   \n",
       "3         3  1646209462036_PTT02R    bbs    Ptt       Stock   \n",
       "4         4  1646217643615_PTT02R    bbs    Ptt       Stock   \n",
       "...     ...                   ...    ...    ...         ...   \n",
       "1345   1856  1706688439506_PTT02R    bbs    Ptt       Stock   \n",
       "1346   1857  1706693007636_PTT02R    bbs    Ptt       Stock   \n",
       "1347   1858  1706706929898_PTT02R    bbs    Ptt       Stock   \n",
       "1348   1859  1706708637116_PTT02R    bbs    Ptt       Stock   \n",
       "1349   1860  1706710789315_PTT02R    bbs    Ptt       Stock   \n",
       "\n",
       "                    post_time                          title       author  \\\n",
       "0     2022-03-01 18:56:10.000          [情報] 2323 中環處分有價證券(發哥  addy7533967   \n",
       "1     2022-03-01 21:11:34.000  [新聞] 聯發科全力搶攻5G手機 天璣8100、天璣800      Ruo5566   \n",
       "2     2022-03-02 14:52:08.000             [情報] 0302上市投信買賣超排行     MOMO0478   \n",
       "3     2022-03-02 16:20:45.000             [情報] 0302上市外資買賣超排行     MOMO0478   \n",
       "4     2022-03-02 18:37:59.000           [情報] 0302八大公股銀行買賣超排行  addy7533967   \n",
       "...                       ...                            ...          ...   \n",
       "1345  2024-01-31 16:06:51.000            [情報] 0131 上市外資買賣超排行      paidzou   \n",
       "1346  2024-01-31 17:22:06.000       [新聞]  〈聯發科法說〉去年Q4獲利登五季新高    xlaws1987   \n",
       "1347  2024-01-31 21:14:40.000     [新聞] 聯發科旗下奕微科車規晶片、MEMS 相繼問    NerfMePls   \n",
       "1348  2024-01-31 21:42:06.000          [情報] 113年01月31日信用交易統計   steward135   \n",
       "1349  2024-01-31 22:14:25.000      [新聞] 胡潤世界500強企業公布！台積電「被」列   lspss93009   \n",
       "\n",
       "                                                content  \\\n",
       "0     1. 標題：公告本公司處分有價證券\\n\\n2. 來源：公開資訊觀測站\\n\\n3. 網址：ht...   \n",
       "1     聯發科全力搶攻5G手機 天璣8100、天璣8000登場\\nhttps://ec.ltn.co...   \n",
       "2     買超                    賣超\\n名次    股票名稱        超張...   \n",
       "3     買超                            賣超\\n名次    股票名稱  ...   \n",
       "4     手機介面圖片好讀版：\\n\\n\\n以下資訊依張數排列\\n買超                 ...   \n",
       "...                                                 ...   \n",
       "1345  1. 標題：0131 上市外資買賣超排行\\n\\n2. 來源：TWSE\\n\\n3. 網址：ht...   \n",
       "1346  原文連結：https://reurl.cc/lgzzXd\\n發布時間： 2024-01-31...   \n",
       "1347  原文標題：聯發科旗下奕微科車規晶片、MEMS 相繼問世 搶攻全球車廠訂單\\n\\n原文連結：h...   \n",
       "1348  1. 標題：113年01月31日信用交易統計\\n2. 來源：臺灣證券交易所、證券櫃檯買賣中心...   \n",
       "1349  原文標題：胡潤世界500強企業公布！台積電「被」列中國第一　台灣3家入榜\\n\\n原文連結：h...   \n",
       "\n",
       "                                               page_url stock_name  category  \n",
       "0     http://www.ptt.cc/bbs/Stock/M.1646132174.A.167...        聯發科        -1  \n",
       "1     http://www.ptt.cc/bbs/Stock/M.1646140301.A.983...        聯發科        -1  \n",
       "2     http://www.ptt.cc/bbs/Stock/M.1646203930.A.51D...        聯發科        -1  \n",
       "3     http://www.ptt.cc/bbs/Stock/M.1646209247.A.D2B...        聯發科        -1  \n",
       "4     http://www.ptt.cc/bbs/Stock/M.1646217502.A.CF6...        聯發科        -1  \n",
       "...                                                 ...        ...       ...  \n",
       "1345  http://www.ptt.cc/bbs/Stock/M.1706688414.A.A04...        聯發科         1  \n",
       "1346  http://www.ptt.cc/bbs/Stock/M.1706692928.A.B95...        聯發科         1  \n",
       "1347  http://www.ptt.cc/bbs/Stock/M.1706706882.A.E24...        聯發科         1  \n",
       "1348  http://www.ptt.cc/bbs/Stock/M.1706708529.A.276...        聯發科         1  \n",
       "1349  http://www.ptt.cc/bbs/Stock/M.1706710467.A.F21...        聯發科         1  \n",
       "\n",
       "[1350 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [upSet, downSet]\n",
    "fullSet = pd.concat(frames).sort_values('post_time').reset_index()\n",
    "fullSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clearSentence(sentence):\n",
    "    return re.sub(r'[^\\u4e00-\\u9fa5]+', '', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dict_docs(docsFrame):\n",
    "  df_counter=Counter()\n",
    "\n",
    "  for index, row in docsFrame.iterrows():\n",
    "      query = row['title'] + ' ' + row['content']\n",
    "      df_tmp = Counter()\n",
    "      sentence_list = utils.short_sentence(query)\n",
    "\n",
    "      for item in sentence_list:\n",
    "        item = clearSentence(item)\n",
    "        result_cut = monpa.cut(item) \n",
    "        for term in result_cut:\n",
    "          term = term.strip()\n",
    "          if (len(term) > 1):\n",
    "            if(df_tmp[term] == 0): \n",
    "              df_tmp[term] = 1 \n",
    "      df_counter += df_tmp\n",
    "  return df_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_term_docs(docsRow):\n",
    "  tf_counter=Counter()\n",
    "\n",
    "  for index, row in docsRow.iterrows():\n",
    "      query = row['title'] + ' ' + row['content']\n",
    "      sentence_list = utils.short_sentence(query)\n",
    "      \n",
    "      for item in sentence_list:\n",
    "        item = clearSentence(item)\n",
    "        result_cut = monpa.cut(item) \n",
    "        for term in result_cut:\n",
    "          term = term.strip()\n",
    "          if (len(term) > 1):\n",
    "            tf_counter[term]+=1\n",
    "  return tf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likeliratio_selection(docsFrame, trainCateg=[], nFeatrues=500):\n",
    "    tfDocsList = {}\n",
    "    vocabDict = cal_dict_docs(docsFrame)\n",
    "    ScoreList = {}\n",
    "        \n",
    "    for i in range(len(docsFrame)):\n",
    "        tfDocsList[i] = cal_term_docs(docsFrame.loc[[i]])\n",
    "        \n",
    "    for term in vocabDict:\n",
    "        Score = 0\n",
    "        for categ in trainCateg:\n",
    "            real = np.zeros((2,2))\n",
    "            for index, row in docsFrame.iterrows():\n",
    "                if (row['category'] == trainCateg[categ]):\n",
    "                    if (term in tfDocsList[index]):\n",
    "                        real[0][0] += 1\n",
    "                    else:\n",
    "                        real[0][1] += 1\n",
    "                else:\n",
    "                    if (term in tfDocsList[index]):\n",
    "                        real[1][0] += 1\n",
    "                    else:\n",
    "                        real[1][1] += 1\n",
    "            sumTotal = real.sum()\n",
    "            pt = (real[0][0] + real[1][0]) / sumTotal\n",
    "            p1 = real[0][0] / (real[0][0] + real[0][1])\n",
    "            p2 = real[1][0] / (real[1][0] + real[1][1])\n",
    "            lambdaVal = ((pt ** real[0][0]) * ((1 - pt) ** real[0][1]) * (pt ** real[1][0]) * ((1 - pt) ** real[1][1])) / ((p1 ** real[0][0]) * ((1 - p1) ** real[0][1]) * (p2 ** real[1][0]) * ((1 - p2) ** real[1][1]))\n",
    "            Score += -2 * math.log(lambdaVal)\n",
    "        ScoreList[term] = Score\n",
    "    selected = sorted(ScoreList, key=ScoreList.get, reverse=True)[:nFeatrues]\n",
    "    totalList = sorted(ScoreList, key=ScoreList.get, reverse=True)\n",
    "    return selected, totalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_30704\\2821600081.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambdaVal = ((pt ** real[0][0]) * ((1 - pt) ** real[0][1]) * (pt ** real[1][0]) * ((1 - pt) ** real[1][1])) / ((p1 ** real[0][0]) * ((1 - p1) ** real[0][1]) * (p2 ** real[1][0]) * ((1 - p2) ** real[1][1]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_keywords, total_keywords = likeliratio_selection(fullSet, trainCateg = [1, -1], nFeatrues = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "with open('result_keywords.pkl', 'wb') as file: \n",
    "    pickle.dump(result_keywords, file)\n",
    "with open('total_keywords.pkl', 'wb') as file: \n",
    "    pickle.dump(total_keywords, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_keywords.pkl', 'rb') as f:\n",
    "    result_keywords = pickle.load(f)\n",
    "with open('total_keywords.pkl', 'rb') as f:\n",
    "    total_keywords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_keywords = total_keywords[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將全部看漲與看跌隨機分類，並預測準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集大小： 1080\n",
      "測試集大小： 270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 合併上漲和下跌的資料集\n",
    "df = pd.concat([upSet, downSet], ignore_index=True)\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"訓練集大小：\", len(train_df))\n",
    "print(\"測試集大小：\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立訓練資料空間向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenStr_list = []\n",
    "for i in list(train_df.index):\n",
    "    try:\n",
    "        txt = clearSentence(train_df['content'][i])\n",
    "        sentence_list = utils.short_sentence(txt)\n",
    "        tokenStr = str()\n",
    "        for sentence in sentence_list:\n",
    "            tokens = monpa.cut(sentence)\n",
    "            tokenStr += ' '.join(tokens)\n",
    "        train_tokenStr_list.append(tokenStr)\n",
    "    except:\n",
    "        train_tokenStr_list.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一一一</th>\n",
       "      <th>一一三</th>\n",
       "      <th>一一九五億</th>\n",
       "      <th>一一二</th>\n",
       "      <th>一一定</th>\n",
       "      <th>一三七六</th>\n",
       "      <th>一三四六六七</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一世代</th>\n",
       "      <th>一二</th>\n",
       "      <th>...</th>\n",
       "      <th>龍華城健</th>\n",
       "      <th>龍豐達科</th>\n",
       "      <th>龍順德</th>\n",
       "      <th>龍頭</th>\n",
       "      <th>龍頭廠</th>\n",
       "      <th>龍頭料</th>\n",
       "      <th>龍頭標</th>\n",
       "      <th>龍頭股</th>\n",
       "      <th>龐大</th>\n",
       "      <th>龔明鑫</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 27086 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      一一一  一一三  一一九五億  一一二  一一定  一三七六  一三四六六七  一下子  一世代   一二  ...  龍華城健  龍豐達科  \\\n",
       "0     0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1     0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2     0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3     0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4     0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "...   ...  ...    ...  ...  ...   ...     ...  ...  ...  ...  ...   ...   ...   \n",
       "1075  0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1076  0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1077  0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1078  0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1079  0.0  0.0    0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "      龍順德   龍頭      龍頭廠  龍頭料  龍頭標  龍頭股   龐大  龔明鑫  \n",
       "0     0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.07627  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...      ...  ...  ...  ...  ...  ...  \n",
       "1075  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "1076  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "1077  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "1078  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "1079  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1080 rows x 27086 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "X_train = vectorizer.fit_transform(train_tokenStr_list)\n",
    "X_train = pd.DataFrame(X_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 為避免太多建立的矩陣太稀疏，影響後續預測的效率，故此做chi-square方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一半</th>\n",
       "      <th>一段</th>\n",
       "      <th>一陣子</th>\n",
       "      <th>三成</th>\n",
       "      <th>三成分股</th>\n",
       "      <th>三福化</th>\n",
       "      <th>三陽</th>\n",
       "      <th>三陽工業</th>\n",
       "      <th>上品</th>\n",
       "      <th>上季</th>\n",
       "      <th>...</th>\n",
       "      <th>高檔</th>\n",
       "      <th>鮑爾</th>\n",
       "      <th>鴻海元大</th>\n",
       "      <th>鴻海南電</th>\n",
       "      <th>鴻海廣達聯電</th>\n",
       "      <th>鴻海晶豪科</th>\n",
       "      <th>鴻海雄獅</th>\n",
       "      <th>黃仁勳</th>\n",
       "      <th>黃金粽</th>\n",
       "      <th>點擊者</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       一半   一段  一陣子   三成  三成分股  三福化   三陽  三陽工業   上品   上季  ...   高檔   鮑爾  鴻海元大  \\\n",
       "0     0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1     0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "4     0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "...   ...  ...  ...  ...   ...  ...  ...   ...  ...  ...  ...  ...  ...   ...   \n",
       "1075  0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1076  0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1077  0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1078  0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1079  0.0  0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "\n",
       "      鴻海南電  鴻海廣達聯電  鴻海晶豪科  鴻海雄獅  黃仁勳  黃金粽  點擊者  \n",
       "0      0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "1      0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "2      0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "3      0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "4      0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "...    ...     ...    ...   ...  ...  ...  ...  \n",
       "1075   0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "1076   0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "1077   0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "1078   0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "1079   0.0     0.0    0.0   0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1080 rows x 1000 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df['category']\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k = 1000)\n",
    "chi2_selector.fit(X_train, y_train)\n",
    "kbest_vocabs = X_train.columns[chi2_selector.get_support()]\n",
    "X_train = X_train[kbest_vocabs]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立訓練資料空間向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenStr_list = []\n",
    "for i in list(test_df.index):\n",
    "    try:\n",
    "        txt = clearSentence(test_df['content'][i])\n",
    "        sentence_list = utils.short_sentence(txt)\n",
    "        tokenStr = str()\n",
    "        for sentence in sentence_list:\n",
    "            tokens = monpa.cut(sentence)\n",
    "            tokenStr += ' '.join(tokens)\n",
    "        test_tokenStr_list.append(tokenStr)\n",
    "    except:\n",
    "        test_tokenStr_list.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一半</th>\n",
       "      <th>一段</th>\n",
       "      <th>一陣子</th>\n",
       "      <th>三成</th>\n",
       "      <th>三成分股</th>\n",
       "      <th>三福化</th>\n",
       "      <th>三陽</th>\n",
       "      <th>三陽工業</th>\n",
       "      <th>上品</th>\n",
       "      <th>上季</th>\n",
       "      <th>...</th>\n",
       "      <th>高檔</th>\n",
       "      <th>鮑爾</th>\n",
       "      <th>鴻海元大</th>\n",
       "      <th>鴻海南電</th>\n",
       "      <th>鴻海廣達聯電</th>\n",
       "      <th>鴻海晶豪科</th>\n",
       "      <th>鴻海雄獅</th>\n",
       "      <th>黃仁勳</th>\n",
       "      <th>黃金粽</th>\n",
       "      <th>點擊者</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      一半  一段  一陣子   三成  三成分股  三福化   三陽  三陽工業   上品   上季  ...   高檔  鮑爾  鴻海元大  \\\n",
       "0    0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "1    0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "2    0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "3    0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "4    0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "..   ...  ..  ...  ...   ...  ...  ...   ...  ...  ...  ...  ...  ..   ...   \n",
       "265  0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "266  0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "267  0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "268  0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "269  0.0   0  0.0  0.0     0    0  0.0   0.0  0.0  0.0  ...  0.0   0   0.0   \n",
       "\n",
       "     鴻海南電  鴻海廣達聯電  鴻海晶豪科  鴻海雄獅  黃仁勳  黃金粽  點擊者  \n",
       "0       0       0      0     0  0.0    0  0.0  \n",
       "1       0       0      0     0  0.0    0  0.0  \n",
       "2       0       0      0     0  0.0    0  0.0  \n",
       "3       0       0      0     0  0.0    0  0.0  \n",
       "4       0       0      0     0  0.0    0  0.0  \n",
       "..    ...     ...    ...   ...  ...  ...  ...  \n",
       "265     0       0      0     0  0.0    0  0.0  \n",
       "266     0       0      0     0  0.0    0  0.0  \n",
       "267     0       0      0     0  0.0    0  0.0  \n",
       "268     0       0      0     0  0.0    0  0.0  \n",
       "269     0       0      0     0  0.0    0  0.0  \n",
       "\n",
       "[270 rows x 1000 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_df['category']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "X_test = vectorizer.fit_transform(test_tokenStr_list)\n",
    "X_test = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "X_test = X_test.reindex(kbest_vocabs, axis=1, fill_value=0)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立預測模型-GDBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990740740740741"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=7, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 55  68]\n",
      " [ 39 108]]\n",
      "Accuracy: 0.6037037037037037\n"
     ]
    }
   ],
   "source": [
    "# 進行預測\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# 評估準確率\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立預測模型-NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  9 114]\n",
      " [  7 140]]\n",
      "Accuracy: 0.5518518518518518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Naive Bayes 分類器\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "#  使用訓練數據訓練分類器\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#  使用分類器進行預測\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# 評估準確率\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立預測模型-SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 37  86]\n",
      " [ 27 120]]\n",
      "Accuracy: 0.5814814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 创建支持向量机分类器\n",
    "classifier = SVC(kernel='linear')\n",
    "\n",
    "# 使用训练数据和标签训练分类器\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# 使用训练好的分类器对测试数据进行预测\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立預測模型-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[52 71]\n",
      " [52 95]]\n",
      "Accuracy: 0.5444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# decision tree 分類器\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立預測模型-Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 50  73]\n",
      " [ 35 112]]\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSet['post_time'] = pd.to_datetime(fullSet['post_time']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2022-03-03 ~ 2022-05-31\n",
      "Test: 2022-06-01 ~ 2022-06-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "2022/06/01\n",
      "Train: 2022-04-02 ~ 2022-06-30\n",
      "Test: 2022-07-01 ~ 2022-07-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066]\n",
      "[ 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "2022/07/01\n",
      "Train: 2022-05-03 ~ 2022-07-31\n",
      "Test: 2022-08-01 ~ 2022-08-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426]\n",
      "[ 1 -1  1 -1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1]\n",
      "2022/08/01\n",
      "Train: 2022-06-03 ~ 2022-08-31\n",
      "Test: 2022-09-01 ~ 2022-09-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1]\n",
      "2022/09/01\n",
      "Train: 2022-07-03 ~ 2022-09-30\n",
      "Test: 2022-10-01 ~ 2022-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  1 -1 -1  1\n",
      " -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "2022/10/01\n",
      "Train: 2022-08-03 ~ 2022-10-31\n",
      "Test: 2022-11-01 ~ 2022-11-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1]\n",
      "2022/11/01\n",
      "Train: 2022-09-02 ~ 2022-11-30\n",
      "Test: 2022-12-01 ~ 2022-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2022/12/01\n",
      "Train: 2022-10-03 ~ 2022-12-31\n",
      "Test: 2023-01-01 ~ 2023-01-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/01/01\n",
      "Train: 2022-11-03 ~ 2023-01-31\n",
      "Test: 2023-02-01 ~ 2023-02-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/02/01\n",
      "Train: 2022-12-01 ~ 2023-02-28\n",
      "Test: 2023-03-01 ~ 2023-03-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1]\n",
      "2023/03/01\n",
      "Train: 2023-01-01 ~ 2023-03-31\n",
      "Test: 2023-04-01 ~ 2023-04-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1]\n",
      "2023/04/01\n",
      "Train: 2023-01-31 ~ 2023-04-30\n",
      "Test: 2023-05-01 ~ 2023-05-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "2023/05/01\n",
      "Train: 2023-03-03 ~ 2023-05-31\n",
      "Test: 2023-06-01 ~ 2023-06-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1  1\n",
      "  1]\n",
      "2023/06/01\n",
      "Train: 2023-04-02 ~ 2023-06-30\n",
      "Test: 2023-07-01 ~ 2023-07-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384]\n",
      "[-1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1  1]\n",
      "2023/07/01\n",
      "Train: 2023-05-03 ~ 2023-07-31\n",
      "Test: 2023-08-01 ~ 2023-08-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/08/01\n",
      "Train: 2023-06-03 ~ 2023-08-31\n",
      "Test: 2023-09-01 ~ 2023-09-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681, 0.03508771929824561]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1  1]\n",
      "2023/09/01\n",
      "Train: 2023-07-03 ~ 2023-09-30\n",
      "Test: 2023-10-01 ~ 2023-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681, 0.03508771929824561, 0.9777777777777777]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/10/01\n",
      "Train: 2023-08-03 ~ 2023-10-31\n",
      "Test: 2023-11-01 ~ 2023-11-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681, 0.03508771929824561, 0.9777777777777777, 0.9137931034482759]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/11/01\n",
      "Train: 2023-09-02 ~ 2023-11-30\n",
      "Test: 2023-12-01 ~ 2023-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681, 0.03508771929824561, 0.9777777777777777, 0.9137931034482759, 0.38461538461538464]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2023/12/01\n",
      "Train: 2023-10-03 ~ 2023-12-31\n",
      "Test: 2024-01-01 ~ 2024-01-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.14516129032258066, 0.8524590163934426, 0.8125, 0.23809523809523808, 0.12244897959183673, 0.35294117647058826, 1.0, 0.46808510638297873, 0.2289156626506024, 0.6486486486486487, 0.015384615384615385, 0.16326530612244897, 0.5384615384615384, 0.7021276595744681, 0.03508771929824561, 0.9777777777777777, 0.9137931034482759, 0.38461538461538464, 0.7236842105263158]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1]\n",
      "2024/01/01\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.date(2022, 6, 1)\n",
    "end_date = datetime.date(2024, 1, 31)\n",
    "current_date = start_date\n",
    "Score = []\n",
    "result = []\n",
    "Date = []\n",
    "while current_date <= end_date - datetime.timedelta(days=26):\n",
    "    date_lst = []\n",
    "    train_startDate = current_date - datetime.timedelta(days=90)\n",
    "    train_endDate = current_date - datetime.timedelta(days=1)\n",
    "    print(f\"Train: {train_startDate} ~ {train_endDate}\")\n",
    "    test_startDate = current_date\n",
    "    if current_date.strftime(\"%m\") in ['01','03','05','07','08','10','12']:\n",
    "        test_endDate = test_startDate + datetime.timedelta(days=30)\n",
    "    elif current_date.strftime(\"%m\") in ['04','06','09','11']:\n",
    "        test_endDate = test_startDate + datetime.timedelta(days=29)\n",
    "    else:\n",
    "        test_endDate = test_startDate + datetime.timedelta(days=27)\n",
    "    print(f\"Test: {test_startDate} ~ {test_endDate}\")\n",
    "\n",
    "    #train\n",
    "    train_tokenStr_list = []\n",
    "\n",
    "    for i in list(fullSet[fullSet['post_time'].between(train_startDate, train_endDate)].index):\n",
    "        try:\n",
    "            sentence_list = utils.short_sentence(fullSet['content'][i])\n",
    "            tokenStr = str()\n",
    "            for sentence in sentence_list:\n",
    "                sentence = clearSentence(sentence)\n",
    "                tokens = monpa.cut(sentence)\n",
    "                tokenStr += ' '.join(tokens)\n",
    "            train_tokenStr_list.append(tokenStr)\n",
    "        except:\n",
    "            train_tokenStr_list.append('')\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "    y_train = fullSet[fullSet['post_time'].between(train_startDate, train_endDate)]['category']\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_tokenStr_list)\n",
    "    X_train = pd.DataFrame(X_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    X_train[[col for col in X_train.columns if col in result_keywords]]\n",
    "\n",
    "    X_trainN = pd.DataFrame(index=range(0,len(X_train)), columns=result_keywords).fillna(0)\n",
    "    X_trainN[[col for col in X_train.columns if col in result_keywords]] = X_train[[col for col in X_train.columns if col in result_keywords]]\n",
    "\n",
    "    #test\n",
    "    test_tokenStr_list = []\n",
    "    date_list = []\n",
    "    for i in list(fullSet[fullSet['post_time'].between(test_startDate, test_endDate)].index):\n",
    "        date_list.append(fullSet['post_time'][i])\n",
    "        try:\n",
    "            sentence_list = utils.short_sentence(fullSet['content'][i])\n",
    "            tokenStr = str()\n",
    "            for sentence in sentence_list:\n",
    "                sentence = clearSentence(sentence)\n",
    "                tokens = monpa.cut(sentence)\n",
    "                tokenStr += ' '.join(tokens)\n",
    "            test_tokenStr_list.append(tokenStr)\n",
    "        except:\n",
    "            test_tokenStr_list.append('')\n",
    "    \n",
    "    y_test = fullSet[fullSet['post_time'].between(test_startDate, test_endDate)]['category']\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "    X_test = vectorizer.fit_transform(test_tokenStr_list)\n",
    "    X_test = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    X_testN = pd.DataFrame(index=range(0,len(X_test)), columns=result_keywords).fillna(0)\n",
    "    X_testN[[col for col in X_test.columns if col in result_keywords]] = X_test[[col for col in X_test.columns if col in result_keywords]]\n",
    "\n",
    "    #model\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X_trainN, y_train)\n",
    "    \n",
    "    Score.append(clf.score(X_testN, y_test))\n",
    "    print(Score)\n",
    "    Predict = clf.predict(X_testN)\n",
    "    print(Predict)\n",
    "    result.append(Predict)\n",
    "    Date.append(date_list)\n",
    "\n",
    "    print(current_date.strftime(\"%Y/%m/%d\"))\n",
    "    if current_date.strftime(\"%m\") in ['01','03','05','07','08','10','12']:\n",
    "        current_date = current_date + datetime.timedelta(days=31)\n",
    "    elif current_date.strftime(\"%m\") in ['04','06','09','11']:\n",
    "        current_date = current_date + datetime.timedelta(days=30)\n",
    "    else:\n",
    "        current_date = current_date + datetime.timedelta(days=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測每日漲跌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_predictions(time_list, prediction_list):\n",
    "    daily_results = {}\n",
    "    \n",
    "    for time, prediction in zip(time_list, prediction_list):\n",
    "        # 將日期轉換為每日日期\n",
    "        daily_date = time\n",
    "        \n",
    "        # 如果這一天還沒有預測結果的列表，就建立一個新的列表\n",
    "        if daily_date not in daily_results:\n",
    "            #print(daily_date)\n",
    "            daily_results[daily_date] = []\n",
    "        \n",
    "        # 將預測結果加入到該日期的列表中\n",
    "        daily_results[daily_date].append(prediction)\n",
    "    \n",
    "    # 進行投票\n",
    "    daily_votes = {}\n",
    "    for date, predictions in daily_results.items():\n",
    "        # 計算每個日期的投票結果\n",
    "        positive_count = sum(1 for result in predictions if result == 1)\n",
    "        negative_count = sum(1 for result in predictions if result == -1)\n",
    "        \n",
    "        # 決定每日的投票結果\n",
    "        if positive_count > negative_count:\n",
    "            daily_votes[date] = 1\n",
    "        elif positive_count < negative_count:\n",
    "            daily_votes[date] = -1\n",
    "        else:\n",
    "            # 如果正負票數相等，你可以採取一個預設的決策，比如0或者None\n",
    "            daily_votes[date] = 0\n",
    "    \n",
    "    return daily_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat Date\n",
    "Date = [item for sublist in Date for item in sublist]\n",
    "result = [item for sublist in result for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_votes = daily_predictions(Date, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出所有交易日\n",
    "trade_days = sorted(transaction_df.loc[(transaction_df['stock_name']=='聯發科')]['date'])\n",
    "#turn elements in trade_days to date\n",
    "trade_days = pd.to_datetime(trade_days).date\n",
    "trade_days = trade_days[62:]\n",
    "\n",
    "#if a date it not in daily_votes, add it to daily_votes and set its value to 0\n",
    "for date in trade_days:\n",
    "    if date not in daily_votes:\n",
    "        daily_votes[date] = 0\n",
    "\n",
    "#if a date is in daily_votes but not in trade_days, remove it from daily_votes\n",
    "for date in list(daily_votes.keys()):\n",
    "    if date not in trade_days:\n",
    "        daily_votes.pop(date)\n",
    "#trade_days = [datetime.datetime.strptime(date[:10], \"%Y-%m-%d\") for date in trade_days]\n",
    "#trade_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測結果輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_list = [(str(key), value) for key, value in daily_votes.items()]\n",
    "vote_list = sorted(vote_list, key = lambda x: x[0])\n",
    "with open('Prediction_20220601_20240131_PTT.csv', 'w') as file:\n",
    "    for key, value in vote_list:\n",
    "        file.write('%s,%s\\n' % (key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
